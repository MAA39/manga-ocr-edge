# トラブルシューティングログ: 課題と解決策

Manga OCR APIの開発中に、Cloudflare Workers AIインターフェースに関連するいくつかの特定のエラーに遭遇しました。

## エラー 1: `AiError: 3030`
**メッセージ:** `Unable to add image when there are no user-supplied nor system-supplied messages.`
（ユーザーまたはシステムからのメッセージがない場合、画像を追加できません。）

### 状況
Llama 3.2 Visionモデルに対して、標準的なチャット形式を使用してリクエストを送信しようとしたところ、モデルが構造を拒否しました。

### 原因
最初に使用した入力形式では、`messages` と並列に `image` プロパティを配置していましたが、モデルのバリデーターがこれを受け入れなかったか、あるいはメッセージの内容構造がマルチモーダルリクエストとして無効でした。
具体的には以下を試しました：
```javascript
// 失敗したコード
messages: [{ role: 'user', content: '...' }],
image: [...]
```
エラー内容は、システムがテキストメッセージと画像コンテキストを「接続」できていないことを示唆していました。

### 解決策
Workers AIのVisionモデルでサポートされている省略記法である、`prompt` + `image` の組み合わせを使用するように入力を簡素化しました。

## エラー 2: `AiError: 5021`
**メッセージ:** `The estimated number of input and maximum output tokens (1032974) exceeded this model context window limit (128000).`
（推定入力トークン数と最大出力トークン数の合計(1032974)が、このモデルのコンテキストウィンドウ制限(128000)を超えました。）

### 状況
3030エラーを修正した後、1.1MBの画像を送信した際にトークン制限エラーが発生しました。

### 原因
これは、`messages` 形式が非テキストデータをどう扱うか、そして `image` 引数がどう扱うかについての重大な誤解によるものでした。
`content` 配列内に `{ type: 'image', image: ... }` を使って画像を埋め込もうとした際、システムは生のバイト配列をテキストトークン（整数の羅列）として処理してしまったようです。結果として、1バイトあたり約1トークンとして計算され、膨大なトークン数（約100万トークン）になってしまいました。
Llama 3.2のコンテキストウィンドウは128kしかないため、100万トークンは即座にクラッシュを引き起こしました。

### 解決策
入力オブジェクトのトップレベルに明示的な `image` 引数を配置する方法に切り替えました。
```javascript
// 成功したコード
{
  prompt: "...",
  image: [...uint8Array]
}
```
この方法で渡すと、Workers AIは画像を正しく視覚トークン（生のバイト数よりもはるかに少ない）としてエンコードし、コンテキストウィンドウ内に十分に収めることができました。

## 一般的な認証の問題
**症状:** `Failed to fetch auth token: 400 Bad Request` または無期限の保留（pending）状態。
**原因:** ローカルの `wrangler` セッションが期限切れになったか、無効になっていました。
**解決策:**
1. 開発サーバーを停止。
2. `npx wrangler login` を実行し、ブラウザ経由で再認証。
3. `npm run dev` を再起動。
