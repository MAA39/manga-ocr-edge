# 視覚的物語の再構築：漫画画像からのノベライズ生成における最先端技術、モデルアーキテクチャ、およびプロンプトエンジニアリングの包括的調査

## エグゼクティブサマリー
本報告書は、漫画の画像を入力とし、単なる光学的文字認識（OCR）や台詞抽出を超えて、文学的な豊かさを持つテキスト表現（ノベライズ）を生成するための技術的ランドスケープに関する網羅的な調査結果である。デジタルヒューマニティーズとマルチモーダルAIの交差点に位置するこの課題は、視覚情報と言語情報の高度な統合、および文脈に依存した推論能力を必要とする。

本調査では、MagiやMangaLMMといった漫画特化型の最先端モデル、Re:VerseやManga109などの評価ベンチマーク、そしてLlama 3.2 VisionやGPT-4oなどの汎用大規模マルチモーダルモデル（LMM）を物語生成に適応させるための手法を詳細に分析した。特に、漫画固有の記号論（漫符、オノマトペ）を文学的な描写に変換するための「記号から意味へ（Semiotics-to-Semantics）」の翻訳プロセスと、コマ間の時間的・因果的ギャップを埋めるためのプロンプトエンジニアリング戦略（Chain-of-Thought、Show Don't Tell）に焦点を当てている。

分析の結果、現在の技術は個々のコマの認識においては人間レベルに達しつつあるものの、長編物語としての整合性を維持する「長期的な文脈保持」と、描かれていない行間を推論する「物語的補完（Narrative Closure）」において依然として課題を抱えていることが明らかになった。本報告書は、これらの課題を克服するための具体的な技術パイプラインを提案し、将来の研究の方向性を示すものである。

## 1. 序論：視覚的物語理解の深化とノベライズの定義

### 1.1 背景：OCRから物語理解（Narrative Understanding）へのパラダイムシフト
従来の漫画画像処理技術は、主としてアーカイブ作成、翻訳支援、または検索インデックスの構築を目的として発展してきた。これらのタスクにおいて求められる主要な能力は、画像内のテキスト領域を正確に特定し、文字コードへ変換するOCR（Optical Character Recognition）技術であった。しかし、生成AIの台頭とともに、ユーザーの要求は「画像にある文字を読む」ことから、「画像が語る物語を理解し、再構成する」ことへと劇的に変化している。これが、本報告書で扱う「ノベライズ生成」の核心である。

漫画をノベライズするという行為は、単に視覚情報を言語情報に置き換える（image-to-text）タスクではない。それは、静止画の連続体から時間、感情、因果関係、そして雰囲気といった「見えない情報」を抽出し、それらを文学的な形式（散文）として再構築する高度な認知的プロセスである。最近の研究では、このプロセスを「深い物語推論（Deep Narrative Reasoning）」と呼び、表面的な物体認識とは区別している。例えば、キャラクターが涙を流している画像を認識するだけでは不十分であり、その涙が「悲しみ」によるものか、「安堵」によるものか、あるいは「悔しさ」によるものかを、前後の文脈（コマ）から推論し、「彼女の瞳から、安堵のあまり熱い雫がこぼれ落ちた」といった具体的な文章に昇華させる必要がある。

### 1.2 漫画の固有性と技術的障壁
漫画、特に日本の漫画（Manga）は、西洋のコミックとは異なる独自の文法と構造を持っており、これが計算機による処理を困難にしている。 第一に、非線形な読み順と複雑なレイアウトが挙げられる。日本の漫画は右から左への読み順を基本とするが、視線誘導は必ずしもジグザグに進むわけではない。大小様々なコマが重なり合い、時にはページ全体を一つのコマとして扱う「ぶち抜き」や、コマの枠線自体が存在しない表現も多用される。これを正しく順序付け（Panel Ordering）できなければ、生成される物語の時系列は破綻する。

第二に、マルチモーダルな情報の密結合である。漫画では、台詞（吹き出し）、独白（四角い枠）、擬音語・擬態語（オノマトペ）、そして背景美術が一体となって情報を伝達する。特にオノマトペは、画像の一部として描画されることが多く、これをテキストとして抽出するだけでなく、その「音」や「状態」が持つ意味的なニュアンス（例：「シーン」という音のない音）を解釈する必要がある。

第三に、漫符（Manpu）と呼ばれる視覚的記号体系の存在である。汗の雫（焦り）、こめかみの青筋（怒り）、鼻提灯（睡眠）といった記号は、物理的な現象ではなく心理的な状態を表すメタファーである。これらを物理的な描写（「顔に水滴がついている」）として処理してしまうと、物語の文脈が失われる。ノベライズにおいては、これらの記号を「焦燥感に駆られ、冷や汗が背中を伝う」といった内面描写に翻訳する能力が求められる。

### 1.3 本報告書の構成と目的
本報告書は、これらの課題を解決し、高品質なノベライズを実現するための技術的枠組みを体系化することを目的とする。第2章では、漫画理解のための基盤となるデータセットと評価指標について詳述する。第3章では、最新のモデルアーキテクチャとその特性を分析する。第4章では、画像から物語を生成するための具体的な技術パイプラインを提案する。第5章および第6章では、生成されるテキストの質を飛躍的に高めるためのプロンプトエンジニアリングと、特殊な視覚言語の翻訳手法について論じる。最後に、第7章で実装戦略と将来展望をまとめる。

## 2. 基盤データセットと評価ベンチマークの分析
AIモデルが漫画の物語を理解するためには、質の高い学習データと、その能力を公正に測定するためのベンチマークが不可欠である。ここでは、現在の研究を支える主要なリソースについて詳細に分析する。

### 2.1 Manga109とその拡張：構造化データの金字塔
Manga109は、漫画研究における最も重要かつ広範に使用されているデータセットである。東京大学の研究チームによって構築され、商業出版された109タイトルの漫画（約21,000ページ）を含み、学術利用のために著者の許諾が得られている点が最大の特徴である。

**アノテーションの深度**: Manga109には、コマ（Panel）、吹き出し（Text Bubble）、キャラクターの身体（Body）、顔（Face）、およびテキスト内容に関する詳細なバウンディングボックス（BB）情報が付与されている。さらに近年の拡張版では、キャラクターの同一性判定（Coreference Resolution）や、コマごとの読み順序に関する情報も追加されている。

**ノベライズへの応用**: このデータセットは、単なる物体検出の訓練だけでなく、「画像とテキストの対応関係」を学習するために極めて重要である。例えば、あるコマの画像特徴量と、そこに含まれるテキスト情報のペアを学習させることで、モデルは「どのような視覚的状況で、どのような台詞が発せられるか」というパターンを獲得する。最新の研究では、Manga109の各ページに対して、大規模言語モデル（LLM）を用いて生成した「シーン説明（キャプション）」や「物語の要約」を付与したManga109Storyといった派生データセットも提案されており、これがノベライズ生成モデルの訓練基盤となっている。

### 2.2 Re:Verse：長編物語理解の厳密な評価
2025年に発表されたRe:Verseベンチマークは、漫画のAI処理における最大の課題であった「長期間の文脈理解」に焦点を当てた画期的な評価セットである。従来のベンチマーク（Manga109など）が主にページ単位やコマ単位の構造解析を重視していたのに対し、Re:Verseは「物語（Narrative）」そのものの理解を問う。

**データセットの構成**: 人気作品『Re:ゼロから始める異世界生活』の漫画版11章分（308パネル）を対象とし、それに対応する原作ライトノベルのテキストをアライメントさせている。これにより、視覚的なコマの連続が、正解となる「小説のテキスト」とどのように対応するかを定量的に評価することが可能となった。

**評価タスクの多様性**: Re:Verseは以下の3つの軸でモデルを評価する。
1.  **物語生成（Story Generation）**: 画像列を入力とし、対応する物語テキストを生成させる。生成されたテキストの流暢さだけでなく、原作のプロットとの整合性が問われる。
2.  **対話のグラウンディング（Dialogue Grounding）**: ある台詞が、どのコマのどのキャラクターによって発せられたかを特定する。これは、ノベライズにおいて「誰が話しているか」を記述するために必須の能力である。
3.  **時間的推論（Temporal Reasoning）**: コマとコマの間の因果関係や時間経過を理解する能力を測る。例えば、キャラクターが倒れているコマの前に、攻撃を受けたコマがある場合、その因果関係を正しく記述できるかを評価する。

**「推論のギャップ（Inferent Gap）」の発見**: Re:Verseを用いた実験により、GPT-4oを含む現在の最先端モデル（SOTA）であっても、長期的なキャラクターの一貫性維持や、非線形な時間構造（回想シーンなど）の理解において、人間と比較して著しい性能低下を示すことが明らかになった。特に、物語の因果関係を推論する能力において、モデルは表面的な視覚情報の羅列に留まる傾向がある。

### 2.3 MangaVQAと文脈理解の深化
MangaVQAは、漫画画像に対する質問応答（Visual Question Answering）タスクを通じて、モデルの文脈理解能力を測定するベンチマークである。

**質問の質**: 「このページの左上のコマで、主人公が驚いている理由は何ですか？」といった、画像内の視覚情報とテキスト情報を統合し、かつ前後の文脈を参照しなければ回答できない高度な質問が含まれている。

**MangaLMMの開発**: このベンチマークと同時に開発されたMangaLMMモデルは、OCRタスクとVQAタスクをマルチタスク学習することで、漫画特有の複雑な文脈を理解する能力を向上させている。

## 3. 最先端モデルとアーキテクチャの技術的詳細
漫画のノベライズを実現するためには、視覚情報を処理するエンコーダと、物語を生成するデコーダ（LLM）の高度な連携が必要である。ここでは、現在利用可能な最も有力なモデルアーキテクチャについて詳述する。

### 3.1 Magi (The Manga Whisperer): 構造解析のスペシャリスト
Magiは、漫画の構造解析において現在最も高性能なモデルの一つであり、ノベライズの前処理段階において決定的な役割を果たす。GitHub上で公開されているバージョン（v1, v2, v3）ごとに機能が拡張されている。

**アーキテクチャ**: Magiは、物体検出モデル（YOLOベースやTransformerベース）を拡張し、漫画特有のクラス（コマ、テキストブロック、キャラクター）を検出するように訓練されている。さらに、検出された要素間の関係性（読み順、話者特定）をグラフ構造として推論する機能を持つ。

**キャラクタークラスタリング**: Magi v2以降の重要な機能として、ページ内あるいはチャプター全体にわたって同一キャラクターを認識・追跡するクラスタリング機能がある。これは、ノベライズにおいて「主人公は〜と言った」「彼女は〜した」といった一貫した主語を用いるために不可欠である。

**Magi v3の革新**: 最新のMagi v3は、因果的言語モデル（Causal Language Model）のアーキテクチャを採用しており、検出やOCRの結果を入力として、直接的に「散文（Prose）」やキャプションを生成する機能が統合された。Florence-2に触発されたこのSequence-to-Sequenceアプローチは、画像とテキストの境界を曖昧にし、よりシームレスなノベライズ生成を可能にしている。

### 3.2 MangaLMM: ドメイン特化型マルチモーダルモデル
MangaLMMは、Qwen2.5-VLなどの高性能なオープンソースLMMをベースに、Manga109データセットを用いてファインチューニングされたモデルである。

**学習戦略**: MangaLMMは、OCRタスク（テキストの読み取り）とVQAタスク（文脈理解）を同時に学習するマルチタスク学習を採用している。これにより、単に文字を読むだけでなく、「どのような状況でその言葉が発せられたか」という文脈依存の読解能力を獲得している。

**性能**: プロプライエタリなモデル（GPT-4Vなど）と比較しても、漫画ドメインにおいてはMangaLMMの方が高いスコアを記録する場合がある。これは、漫画特有の表現（漫符や特殊なコマ割り）に関する知識が、汎用モデルには不足しているためである。

### 3.3 汎用LMMの適応：GPT-4oとLlama 3.2 Vision
汎用モデルは、漫画に特化していないものの、圧倒的な一般的知識と文章生成能力を持つため、適切なプロンプトエンジニアリングと組み合わせることで強力なノベライズツールとなる。

**GPT-4o**: 現在の最高峰モデルであり、Re:Verseベンチマークでもベースラインとして使用されている。複雑な指示（Instruction Following）に従う能力が高く、「村上春樹風の文体で」といったスタイルの指定にも柔軟に対応できる。しかし、APIコストとデータプライバシーの問題が常につきまとう。

**Llama 3.2 Vision (11B/90B)**: Metaが公開したオープンウェイトモデルであり、ローカル環境での実行が可能である点が最大の強みである。128kトークンの長いコンテキストウィンドウを持ち、長編漫画の物語を一貫して保持するのに適している。

**画像処理の制限**: Llama 3.2 Visionは、入力画像の解像度に制限（最大1120x1120ピクセル）がある場合が多く、高解像度の漫画ページ全体を入力すると細部のテキストや描画が潰れてしまう可能性がある。そのため、ページをコマごとに分割（クロッピング）して順次入力するか、高解像度画像をタイル状に分割して入力する「スライディングウィンドウ」方式の実装が必要となる。

### 3.4 軽量・エッジ向けモデル
個人のクリエイターやローカル環境での利用を想定した場合、計算リソースの制約が問題となる。

**Florence-2**: Microsoftが開発した軽量VLMであり、エッジデバイスでも動作可能である。詳細なキャプション生成や物体検出が可能で、Magi v3のアーキテクチャのベースにもなっている。漫画の各コマの状況説明（Dense Captioning）を高速に生成するタスクに適している。

**Moondream**: 非常に軽量（2Bパラメータ以下）なモデルであり、スマートフォンやラップトップでも動作する。精度は大型モデルに劣るが、簡単なシーン説明やOCRの前処理として利用可能である。

## 4. ノベライズ生成のための技術的パイプライン
単一のモデルで画像入力から小説出力を完結させることは、現状の技術レベルでは品質の不安定さを招く。したがって、複数の専門化されたモジュールを組み合わせたパイプラインアプローチが、最も高品質な結果を生むと考えられる。以下に、推奨されるパイプラインの詳細を示す。

### ステップ1: 構造解析とセグメンテーション（Structural Analysis）
まず、漫画ページを意味のある最小単位（コマ）に分解し、物語の順序を確定する。

*   **モデル**: Magi v3 または YOLOv8（漫画用に訓練されたもの）。
*   **処理**: ページの画像を入力し、コマ（Panel）、テキストボックス、キャラクターのバウンディングボックスを取得する。
*   **順序付け**: 取得したコマの座標情報を基に、DAG（有向非巡回グラフ）アルゴリズムを用いて正しい読み順（右→左、上→下）を決定し、各コマにID（Panel_1, Panel_2...）を割り振る。

### ステップ2: 意味情報の抽出とデジタル化（Semantic Extraction）
次に、各コマに含まれる情報をテキストデータとして抽出する。

*   **OCR**: MangaOCRまたはQwen2.5-VLを使用する。Qwen2.5-VLは縦書きや手書き文字の認識精度が高く、さらにテキストの配置情報（レイアウト）も理解できるため、吹き出しの順序を間違えるリスクが低い。
*   **話者特定**: Magiの話者特定機能を使用し、抽出したテキストがどのキャラクターの発話であるかをタグ付けする（例：Character_A: "こんにちは"）。
*   **視覚的タグ付け**: 各コマに対し、VLM（Llama 3.2 VisionやFlorence-2）を用いて、「誰が」「どこで」「何をしているか」「どんな表情か」という詳細なキャプション（Scene Description）を生成する。この際、漫符（汗、青筋など）も検出し、`{emotion: nervous, symbol: sweat_drop}`のような構造化データとして記録する。

### ステップ3: 物語的文脈の構築（Narrative Contextualization）
抽出された断片的な情報を、物語として繋ぎ合わせるための準備を行う。

*   **キャラクターバンクの参照**: 以前のページやチャプターから蓄積された「キャラクターバンク」を参照し、Character_Aが「エミリア」であることを同定する。RAG（Retrieval-Augmented Generation）を用い、現在のシーンに関連する過去の伏線や設定情報を検索してコンテキストに追加する。
*   **ギャップの推論**: Panel_1とPanel_2の間で何が起きたかを推論する。例えば、Panel_1で剣を振り上げ、Panel_2で敵が倒れている場合、「剣を振り下ろし、敵を切り裂いた」というアクションが省略されている。この「行間」を埋めるための中間テキストを生成する。

### ステップ4: 文学的テキスト生成（Literary Generation）
最後に、構造化されたデータと推論されたコンテキストを基に、LLMが小説本文を執筆する。

*   **モデル**: Llama 3.2 Vision (Instruct) または GPT-4o。
*   **入力プロンプト**: 構造化されたシーン情報、台詞、オノマトペの意味、そして「文体」や「視点」を指示するシステムプロンプトを統合して入力する。
*   **出力**: `Scene 1: エミリアは剣を振り上げた。銀色の刀身が月光を反射し...` という形式で物語が出力される。

## 5. プロンプトエンジニアリング：AIを作家に変える技術
モデルの性能を最大限に引き出し、無機質な「説明文」ではなく、読者の感情を揺さぶる「物語」を生成させるためには、高度なプロンプトエンジニアリングが必要不可欠である。ここでは、具体的な戦略とテンプレートを提示する。

### 5.1 ペルソナ設定とシステムプロンプト
AIに対して明確な役割（ペルソナ）を与えることで、生成されるテキストのトーンとスタイルを制御する。

**システムプロンプト例:**
> "You are an award-winning novelist specializing in high-fantasy. Your writing style is immersive, detailed, and character-driven, similar to that of Ursula K. Le Guin or George R.R. Martin. Your task is to adapt the provided visual sequence from a manga into a compelling novel chapter. Do not simply describe the images. Instead, inhabit the characters' perspectives, describe their internal sensations, the atmosphere of the setting, and the flow of time. Use sensory language (sight, sound, smell, touch) to bring the scene to life."

### 5.2 "Show, Don't Tell"（見せて語る）の実装
小説執筆の基本原則である「見せて語る」をAIに実践させるには、具体的な禁止事項と推奨事項を明示する必要がある。

**プロンプト戦略:**
*   **禁止**: "He was angry."（彼は怒っていた）のような直接的な感情ラベルの使用を禁止する。
*   **推奨**: 視覚的な手がかり（漫符や表情）を、身体的な反応や行動として描写するように指示する。
*   **Instruction**: "Convert visual symbols into physical descriptions. A 'vein pop' symbol should be described as 'his jaw tightened, a pulse hammering visibly at his temple.' Sweat drops should be 'a cold bead of perspiration tracing a line down her spine.'"

### 5.3 Chain-of-Thought (CoT) による行間推論
コマ間の省略された情報を補完するために、AIに思考のステップを踏ませる。

**プロンプト構造:**
1.  **Analyze**: 各コマの視覚的要素、台詞、感情を客観的にリストアップせよ。
2.  **Infer Gaps**: コマAからコマBへの移行で、描かれていないが論理的に発生したはずのアクションや時間経過を推論せよ。
3.  **Draft**: 上記の情報を統合し、キャラクターの心理描写を交えてドラフトを作成せよ。
4.  **Polish**: 文体を整え、リズム感のある散文にリライトせよ。 このプロセスを経ることで、AIは単なる「画像の翻訳機」から「物語の語り手」へと昇華する。

### 5.4 構造化プロンプト（Structured Prompting）の活用
特にLlama 3系などのモデルでは、特定のタグ構造を用いることで指示の遵守率が向上する。

**テンプレート（Llama 3形式）:**
```
<|begin_of_text|><|start_header_id|>system<|end_header_id|> {{ 作家ペルソナとタスク定義 }} <|eot_id|><|start_header_id|>user<|end_header_id|> I will provide a sequence of manga panels including OCR text and visual descriptions. [Panel 1]: {Visual: Girl crying, Text: "Goodbye..."} [Panel 2]: {Visual: Boy running after train, Text: "Wait!"}

Task: Write a narrative passage connecting these panels. Focus on the boy's desperation. <|eot_id|><|start_header_id|>assistant<|end_header_id|>
```
このように入力を構造化（JSONやMarkdown形式）することで、モデルは情報を正確にパースしやすくなる。

## 6. 特殊要素の翻訳：記号から文学へ
日本の漫画におけるオノマトペや漫符は、独自の「視覚言語」である。これらを無視すれば物語の臨場感は失われ、そのまま文字にすれば滑稽になる。ここでは、これらの記号を文学的な表現に変換するための具体的なマッピング手法を提案する。

### 6.1 オノマトペ（擬音・擬態語）の記述的翻訳
オノマトペは、音（Giongo）だけでなく、状態（Gitaigo）を表す場合も多い。これらを文脈に応じて適切な英語（または日本語の記述）に変換する辞書をプロンプトに組み込むことが有効である。

**変換テーブルの例:**

| カテゴリ | オノマトペ | 直訳（避けるべき） | 文学的翻訳・描写の例 (English / Japanese) |
| :--- | :--- | :--- | :--- |
| 環境音 | ざーざー | Zaa-zaa / Heavy Rain | "The rain drummed relentlessly against the pavement." / 「雨が激しくアスファルトを叩きつけていた。」 |
| 動作音 | スタスタ | Suta-suta / Walk fast | "She walked briskly, her heels clicking a sharp rhythm." / 「彼女は足早に通り過ぎた。」 |
| 心理状態 | シーン | Silence | "A heavy silence descended upon the room." / 「重苦しい沈黙が場を支配した。」 |
| 心理状態 | ドキドキ | Heartbeat | "Her heart hammered against her ribs like a trapped bird." / 「心臓が早鐘を打ち、胸が痛くなるほどだった。」 |
| 視線 | じーっ | Stare | "He felt the weight of an unblinking gaze boring into him." / 「突き刺さるような視線を感じた。」 |

### 6.2 漫符（Manpu）の感情へのマッピング
漫符は物理的な現象ではないため、心理描写への変換が必要である。

**漫符変換ガイドライン:**
*   **青筋（💢）**: "A vein popped" ではなく、"He grit his teeth, suppressing a surge of irritation."（歯を食いしばり、苛立ちを抑え込んだ）
*   **汗（💦 - 顔の横）**: "Sweat drop" ではなく、"A cold nervous sweat broke out on his brow."（冷や汗が滲んだ）あるいは "He laughed awkwardly, trying to diffuse the tension."（気まずそうに笑った）
*   **縦線（||| - 顔の上）**: "Vertical lines" ではなく、"His face fell, the color draining from his cheeks in despair."（絶望で顔色が青ざめた）

これらの変換ルールをSystem Prompt内の「Translation Rules」セクションに記述することで、モデルは視覚記号を適切な文学表現に自動変換することが可能になる。

## 7. 実装戦略と将来展望
### 7.1 推奨される技術スタックと構成
個人の開発者や研究者が実際にノベライズシステムを構築する場合、以下のスタックが推奨される。

| レイヤー | 推奨技術/モデル | 選定理由 |
| :--- | :--- | :--- |
| 検出・構造化 | **Magi v3** | コマ検出、読み順推定、話者特定を単一モデルで高精度に行える唯一のオープンソリューションであるため。 |
| OCR | **MangaOCR** | 漫画特有のフォントや縦書きに対する堅牢性が最も高い。 |
| 推論・生成 | **Llama 3.2 Vision (11B)** | ローカル（エッジ）での実行が可能で、コストがかからず、かつ128kのコンテキスト長を持つため長編に適している。 |
| オーケストレーション | **LangChain / LlamaIndex** | 複雑なパイプライン管理、RAGによるキャラクターバンクの実装、プロンプトチェーンの制御に必須である。 |
| インフラ | **Cloudflare Workers AI** | サーバーレスでLlama 3.2などのモデルを低遅延で実行でき、スケーラビリティが高い。 |

### 7.2 結論と「推論のギャップ」への挑戦
本調査により、漫画のノベライズは技術的に実現可能な段階に入っていることが確認された。しかし、Re:Verseベンチマークが示したように、AIは依然として物語の「深い因果関係」や「長期的な整合性」において弱点を持つ。 今後の研究開発の焦点は、単なるモデルの大規模化ではなく、「物語構造（Narrative Structure）」を明示的に扱うアーキテクチャの導入にあるだろう。具体的には、物語の進行をグラフ（Narrative Graph）として管理し、各イベントの因果関係を外部メモリとして保持するニューロシンボリック（Neuro-symbolic）なアプローチが、完全なノベライズへの鍵となる可能性が高い。

また、この技術の発展は、視覚障害者に対するアクセシビリティの劇的な向上、漫画コンテンツの新たな言語圏への「翻訳を超えた」文化的輸出、そしてクリエイターのためのプロット支援ツールとしての応用など、広範な社会的インパクトをもたらすことが期待される。
